{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# LAB 3: Prompt Engineering Patterns & Conversation Memory\n",
        "# Google Colab + LangChain + Hugging Face\n",
        "# STEP 1: Install required libraries\n",
        "!pip install -q --upgrade langchain langchain-community transformers accelerate sentencepiece langchain-core"
      ],
      "metadata": {
        "id": "rcZHDY43goRQ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "print(requests.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JXgRJSshIgP",
        "outputId": "4bd74788-2483-4441-f194-73d7829448fa"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.32.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 2: Load Hugging Face LLM (FLAN-T5)\n",
        "from transformers import pipeline\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "\n",
        "hf_pipeline = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=\"google/flan-t5-base\",\n",
        "    max_length=256\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=hf_pipeline)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JomZpaOMhx3E",
        "outputId": "fa0aa086-25e4-49d7-b7dd-1afe3e89c3cc"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 3: Import PromptTemplate and Memory components\n",
        "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "# Try importing memory - handle different LangChain versions\n",
        "try:\n",
        "    from langchain.memory import ConversationBufferMemory\n",
        "    from langchain.chains import ConversationChain\n",
        "except ImportError:\n",
        "    try:\n",
        "        from langchain_community.memory import ConversationBufferMemory\n",
        "        from langchain.chains import ConversationChain\n",
        "    except ImportError:\n",
        "        print(\"Note: Using alternative memory approach\")\n",
        "        ConversationBufferMemory = None\n",
        "        ConversationChain = None"
      ],
      "metadata": {
        "id": "DBd7PLf9iS8G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38823997-d31b-44b9-c15e-83ec2fdf371c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Note: Using alternative memory approach\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a40f59e4",
        "outputId": "a4ba18a7-fce2-4d04-ccd1-aaa9a94d55f1"
      },
      "source": [
        "# STEP 4: Demonstration WITHOUT Memory (Problem)\n",
        "simple_prompt = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"Answer the question:\\n{question}\"\n",
        ")\n",
        "\n",
        "print(\"=== WITHOUT MEMORY ===\")\n",
        "print(\"Q: Who is Steve Jobs?\")\n",
        "print(\"A:\", llm.invoke(simple_prompt.format(\n",
        "    question=\"Who is Steve Jobs?\"\n",
        ")))\n",
        "\n",
        "print(\"\\nQ: What is his wife name?\")\n",
        "print(\"A:\", llm.invoke(simple_prompt.format(\n",
        "    question=\"What is his wife name?\"\n",
        ")))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== WITHOUT MEMORY ===\n",
            "Q: Who is Steve Jobs?\n",
            "A: computer programmer\n",
            "\n",
            "Q: What is his wife name?\n",
            "A: elizabeth harris\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_history = []\n",
        "\n",
        "def ask_with_memory(question):\n",
        "    \"\"\"Ask a question with conversation memory\"\"\"\n",
        "    # Build context from conversation history\n",
        "    history_context = \"\"\n",
        "    if conversation_history:\n",
        "        history_context = \"\\n\\nPrevious conversation:\\n\"\n",
        "        for i, (q, a) in enumerate(conversation_history, 1):\n",
        "            history_context += f\"Q{i}: {q}\\nA{i}: {a}\\n\"\n",
        "\n",
        "    # Create prompt with history\n",
        "    prompt_text = f\"\"\"You are a helpful assistant. Use the conversation history to answer questions.{history_context}\n",
        "\n",
        "Current question: {question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "    # Get response\n",
        "    response = llm.invoke(prompt_text)\n",
        "\n",
        "    # Store in memory\n",
        "    conversation_history.append((question, response))\n",
        "\n",
        "    return response\n",
        "\n",
        "# Clear previous conversation\n",
        "conversation_history = []\n",
        "\n",
        "print(\"=== WITH MEMORY ===\")\n",
        "print(\"Q: Who is Steve Jobs?\")\n",
        "response1 = ask_with_memory(\"Who is Steve Jobs?\")\n",
        "print(\"A:\", response1)\n",
        "\n",
        "print(\"\\nQ: What is his wife name?\")\n",
        "response2 = ask_with_memory(\"What is his wife name?\")\n",
        "print(\"A:\", response2)\n",
        "\n",
        "print(\"\\nQ: What company did he co-found?\")\n",
        "response3 = ask_with_memory(\"What company did he co-found?\")\n",
        "print(\"A:\", response3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhsrZPIWvFFj",
        "outputId": "3a477b94-19c9-40e3-efa3-ca18cfb50843"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== WITH MEMORY ===\n",
            "Q: Who is Steve Jobs?\n",
            "A: Steve Jobs\n",
            "\n",
            "Q: What is his wife name?\n",
            "A: Sarah Michelle Gellar\n",
            "\n",
            "Q: What company did he co-found?\n",
            "A: Apple Inc.\n"
          ]
        }
      ]
    }
  ]
}